<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-16T10:22:52-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Rania Saber</title><subtitle>Summer 2024 DREU Project</subtitle><entry><title type="html">Week 4</title><link href="http://localhost:4000/week3/" rel="alternate" type="text/html" title="Week 4" /><published>2024-10-05T00:00:00-07:00</published><updated>2024-10-05T00:00:00-07:00</updated><id>http://localhost:4000/week3</id><content type="html" xml:base="http://localhost:4000/week3/"><![CDATA[<p>This week I continued with creating visualizations from the MET dataset from last week. I also created a slide deck to present my research findings so far to two faculty members from the University of Utah – Dr. Andrew McNutt and Dr. Kate Isaacs. Professor Fariha set up a meeting with us to discuss the research topic and gain insight from them and guidance towards an experimental study. I gained many good points from them on what to consider when designing an experimental study. Some of these points included: defining my audience, what type of issues and tasks to focus on, what tasks matter the most, what would the error debugging process look like, how can people process dealing with these issues, etc. Overall, it was an insightful meeting to gain a different point of view on the study.</p>

<p>This week’s literature review was on the research paper: View-based Explanations for Graph Neural Networks.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This week I continued with creating visualizations from the MET dataset from last week. I also created a slide deck to present my research findings so far to two faculty members from the University of Utah – Dr. Andrew McNutt and Dr. Kate Isaacs. Professor Fariha set up a meeting with us to discuss the research topic and gain insight from them and guidance towards an experimental study. I gained many good points from them on what to consider when designing an experimental study. Some of these points included: defining my audience, what type of issues and tasks to focus on, what tasks matter the most, what would the error debugging process look like, how can people process dealing with these issues, etc. Overall, it was an insightful meeting to gain a different point of view on the study.]]></summary></entry><entry><title type="html">Week 2</title><link href="http://localhost:4000/week1/" rel="alternate" type="text/html" title="Week 2" /><published>2024-10-05T00:00:00-07:00</published><updated>2024-10-05T00:00:00-07:00</updated><id>http://localhost:4000/week1</id><content type="html" xml:base="http://localhost:4000/week1/"><![CDATA[<p>During this week, I presented my findings from week 1 to Professor Fariha, had a discussion about my observations, and recieved feedback regarding the experiment. Then, I was my given my next task which was to find an unclean dataset and perform the same experiment as last week and document steps, errors encountered, time taken to fix each error, the level of complexity, and what conclusion I drew from each visualization. I pre-experiment reading consisted of DataPrism: Exposing Disconnect between Data and Systems by Sainyam Galhotra, Anna Fariha, et al.</p>

<p>I also met the lab team this week for the first time and was introduced to them during our weekly research literature reviews. Each week on Thursdays, a member of the team signed up for a research paper to present to the group and lead a discussion for. This week’s discussion was on Table-GPT: Table-tuned GPT for Diverse Table Tasks.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[During this week, I presented my findings from week 1 to Professor Fariha, had a discussion about my observations, and recieved feedback regarding the experiment. Then, I was my given my next task which was to find an unclean dataset and perform the same experiment as last week and document steps, errors encountered, time taken to fix each error, the level of complexity, and what conclusion I drew from each visualization. I pre-experiment reading consisted of DataPrism: Exposing Disconnect between Data and Systems by Sainyam Galhotra, Anna Fariha, et al.]]></summary></entry><entry><title type="html">Week 10</title><link href="http://localhost:4000/week10/" rel="alternate" type="text/html" title="Week 10" /><published>2024-10-05T00:00:00-07:00</published><updated>2024-10-05T00:00:00-07:00</updated><id>http://localhost:4000/week10</id><content type="html" xml:base="http://localhost:4000/week10/"><![CDATA[<p>This was my final week of the DREU program. I started wrapping up my experimental study although I was not able to get through all 10 datasets with the given time, I was still able to document the patterns and observations I encountered. I met with professor Fariha for a final time to discuss my research findings and what conclusions I drew. She also gave me insight on research in academia vs research in the industry, which I found was helpful.</p>

<p>Then, I started working on my research paper to gather all my research findings and how data quality issues can affect a user who is trying to use AI-assited tools to create visualizations.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This was my final week of the DREU program. I started wrapping up my experimental study although I was not able to get through all 10 datasets with the given time, I was still able to document the patterns and observations I encountered. I met with professor Fariha for a final time to discuss my research findings and what conclusions I drew. She also gave me insight on research in academia vs research in the industry, which I found was helpful.]]></summary></entry><entry><title type="html">Week 3</title><link href="http://localhost:4000/week2/" rel="alternate" type="text/html" title="Week 3" /><published>2024-10-05T00:00:00-07:00</published><updated>2024-10-05T00:00:00-07:00</updated><id>http://localhost:4000/week2</id><content type="html" xml:base="http://localhost:4000/week2/"><![CDATA[<p>I continued my task from last week, I realized that this took longer than I anticipated due to the dataset I chose. For this experiment, I used The Metropolitan Museum of Art Open Access dataset which included a plethora of issues including missing values, inconsistent information, missing documentation, possible duplication, mixed text, and numeric data. Some visualizations took longer than others to fix and many visualization errors were encountered in the process. I shared my findings with Professor Fariha and we discussed the different types of data issues and how they can affect data visualizations. I read another research paper about data mirages to help me understand the topic more.</p>

<p>We did not have a research paper discussion this week as Thursday was a holiday (4th of July).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I continued my task from last week, I realized that this took longer than I anticipated due to the dataset I chose. For this experiment, I used The Metropolitan Museum of Art Open Access dataset which included a plethora of issues including missing values, inconsistent information, missing documentation, possible duplication, mixed text, and numeric data. Some visualizations took longer than others to fix and many visualization errors were encountered in the process. I shared my findings with Professor Fariha and we discussed the different types of data issues and how they can affect data visualizations. I read another research paper about data mirages to help me understand the topic more.]]></summary></entry><entry><title type="html">Week 9</title><link href="http://localhost:4000/week8/" rel="alternate" type="text/html" title="Week 9" /><published>2024-10-05T00:00:00-07:00</published><updated>2024-10-05T00:00:00-07:00</updated><id>http://localhost:4000/week8</id><content type="html" xml:base="http://localhost:4000/week8/"><![CDATA[<p>I continued with my experimental study and started injecting the next issue into the datset and noting any observations down. I noticed some patterns across the results and shared my observations with professor Fariha during our meeting this week.</p>

<p>This week was the final research paper discussion meeting with the team. It was about the paper “TabEE: Tabular Embeddings Explanations” and was led by one of the grad students on the team.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I continued with my experimental study and started injecting the next issue into the datset and noting any observations down. I noticed some patterns across the results and shared my observations with professor Fariha during our meeting this week.]]></summary></entry><entry><title type="html">Week 5</title><link href="http://localhost:4000/week4/" rel="alternate" type="text/html" title="Week 5" /><published>2024-10-05T00:00:00-07:00</published><updated>2024-10-05T00:00:00-07:00</updated><id>http://localhost:4000/week4</id><content type="html" xml:base="http://localhost:4000/week4/"><![CDATA[<p>I started wrapping up my previous task this week and documenting what common data quality issues I encountered and what type of data visualization issues they produced. I made a connection between each data quality issue and their resulting visualization issues based on the results from the MET dataset. I then created a table to organize the visualizations I previously created and categorized them with the different mirage issues to help me gain a better understanding of each issue. I discussed these findings with professor Fariha during our meeting.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I started wrapping up my previous task this week and documenting what common data quality issues I encountered and what type of data visualization issues they produced. I made a connection between each data quality issue and their resulting visualization issues based on the results from the MET dataset. I then created a table to organize the visualizations I previously created and categorized them with the different mirage issues to help me gain a better understanding of each issue. I discussed these findings with professor Fariha during our meeting.]]></summary></entry><entry><title type="html">Week 6</title><link href="http://localhost:4000/week5/" rel="alternate" type="text/html" title="Week 6" /><published>2024-10-05T00:00:00-07:00</published><updated>2024-10-05T00:00:00-07:00</updated><id>http://localhost:4000/week5</id><content type="html" xml:base="http://localhost:4000/week5/"><![CDATA[<p>It was finally time for the experimental study. My next task was to gather 10 different datasets and inject 1 data quality issue into each dataset. Then, I chose 5 visual tasks for each dataset and observed the visualization errors given the data quality issues. I gathered clean 10 datasets from Kaggle. The first dataset I was working with was the Most Streamed Spotify Songs 2024. The 5 visualizations I chose were: pie chart, word cloud, histogram, heat map, and scatter plot.</p>

<p>This week it was my turn to lead a discussion on a research paper with the lab group. The research paper I presented was Automated Data Visualization from Natural Language via Large Language Models. I read the paper and created a slide deck for it to present. I found it interesting as it related to my research project and I saw similiarities between this paper and my project. I discussed this paper with the lab group and presented to them my current project to draw a connection between this paper any my research.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[It was finally time for the experimental study. My next task was to gather 10 different datasets and inject 1 data quality issue into each dataset. Then, I chose 5 visual tasks for each dataset and observed the visualization errors given the data quality issues. I gathered clean 10 datasets from Kaggle. The first dataset I was working with was the Most Streamed Spotify Songs 2024. The 5 visualizations I chose were: pie chart, word cloud, histogram, heat map, and scatter plot.]]></summary></entry><entry><title type="html">Week 7</title><link href="http://localhost:4000/week6/" rel="alternate" type="text/html" title="Week 7" /><published>2024-10-05T00:00:00-07:00</published><updated>2024-10-05T00:00:00-07:00</updated><id>http://localhost:4000/week6</id><content type="html" xml:base="http://localhost:4000/week6/"><![CDATA[<p>I created a Google Colab link with for the first dataset: the Most Streamed Spotify Songs 2024. I observed the dataset to take note of any pre-existing errors before injecting data issues into the dataset. I noted some encoding errors where some characters were not utf-8. This caused incorrect information within the visualization to be portrayed. These errors were fixed by adding an encoding to the script when reading in the file.</p>

<p>I met with Professor Fariha to discuss any hypotheses I had about which data issues would be more prominent than others and how they would appear in each visualization.</p>

<p>This week’s research paper discussion was on Auto-Formula: Recommend Formulas in Spreadsheets using Learned Table Representations.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I created a Google Colab link with for the first dataset: the Most Streamed Spotify Songs 2024. I observed the dataset to take note of any pre-existing errors before injecting data issues into the dataset. I noted some encoding errors where some characters were not utf-8. This caused incorrect information within the visualization to be portrayed. These errors were fixed by adding an encoding to the script when reading in the file.]]></summary></entry><entry><title type="html">Week 8</title><link href="http://localhost:4000/week7/" rel="alternate" type="text/html" title="Week 8" /><published>2024-10-05T00:00:00-07:00</published><updated>2024-10-05T00:00:00-07:00</updated><id>http://localhost:4000/week7</id><content type="html" xml:base="http://localhost:4000/week7/"><![CDATA[<p>I continued my experimental study and created scripts to inject each data quality error to automate the process and make it quicker. I first created visualizations with the clean data to use for comparison against the visualizations with the injected data issue. I met with Professor Fariha to discuss findings and get feedback.</p>

<p>This week’s research paper discussion was led by me. I created a presentation for the paper “The Battleship Approach to the Low Resource Entity Matching Problem”. I found this paper to be quite interesting as they took the game Battleship and used the strategy of the game to solve low resources enitity matching.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I continued my experimental study and created scripts to inject each data quality error to automate the process and make it quicker. I first created visualizations with the clean data to use for comparison against the visualizations with the injected data issue. I met with Professor Fariha to discuss findings and get feedback.]]></summary></entry><entry><title type="html">Week 1</title><link href="http://localhost:4000/week1/" rel="alternate" type="text/html" title="Week 1" /><published>2020-06-01T00:00:00-07:00</published><updated>2020-06-01T00:00:00-07:00</updated><id>http://localhost:4000/week1</id><content type="html" xml:base="http://localhost:4000/week1/"><![CDATA[<p>For my first week, I had initial meeting with Professor Fariha and went over details of the project and established a schedule to meet twice a week. I read five research papers relating to my project to gain further insight and background information about the topic. My first task was to fix imperfect data visualizations given by ChatGPT using a clean dataset. I used the 911 Emergencies dataset on Kaggle to create 10 different visualizations using Python on Google Colab. Then, I documented my steps and any errors expereinced with the code along the way and found solutions to the errors in the AI-generated code.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[For my first week, I had initial meeting with Professor Fariha and went over details of the project and established a schedule to meet twice a week. I read five research papers relating to my project to gain further insight and background information about the topic. My first task was to fix imperfect data visualizations given by ChatGPT using a clean dataset. I used the 911 Emergencies dataset on Kaggle to create 10 different visualizations using Python on Google Colab. Then, I documented my steps and any errors expereinced with the code along the way and found solutions to the errors in the AI-generated code.]]></summary></entry></feed>